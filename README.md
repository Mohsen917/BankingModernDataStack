<div align="center">

# ğŸ¦ Modern Data Stack â€” Banking Analytics Platform

<img src="https://img.shields.io/badge/Status-Production%20Ready-success?style=for-the-badge" alt="Status"/>
<img src="https://img.shields.io/badge/Python-3.11-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python"/>
<img src="https://img.shields.io/badge/License-MIT-yellow?style=for-the-badge" alt="License"/>

**A fully containerized, end-to-end real-time data pipeline for banking analytics**

_From PostgreSQL â†’ Kafka (CDC) â†’ MinIO â†’ Snowflake â†’ dbt â†’ Power BI_

<br/>

[ğŸš€ Quick Start](#-quick-start) â€¢
[ğŸ“ Architecture](#-architecture) â€¢
[ğŸ› ï¸ Tech Stack](#%EF%B8%8F-tech-stack) â€¢
[ğŸ“ Project Structure](#-project-structure) â€¢
[âš™ï¸ Setup](#%EF%B8%8F-setup-guide)

</div>

---

## âœ¨ Highlights

| Feature                       | Description                                         |
| ----------------------------- | --------------------------------------------------- |
| ğŸ”„ **Real-time CDC**          | Capture data changes instantly via Debezium + Kafka |
| ğŸ—ï¸ **Medallion Architecture** | Bronze â†’ Silver â†’ Gold data layers in Snowflake     |
| ğŸ“¸ **SCD Type 2**             | Track historical changes with dbt snapshots         |
| ğŸ¯ **Orchestration**          | Apache Airflow manages all workflows                |
| ğŸš€ **CI/CD**                  | Automated testing & deployment with GitHub Actions  |
| ğŸ³ **Fully Containerized**    | One command to spin up the entire stack             |

---

## ğŸ“ Architecture

<div align="center">
  <img src="./Screenshot 2025-11-30 104348.png" alt="Modern Data Stack Architecture" width="100%"/>
</div>

### ğŸ“Š Data Flow

```
1ï¸âƒ£ PostgreSQL        â†’  Source banking transactions (customers, accounts, transactions)
2ï¸âƒ£ Debezium + Kafka  â†’  Real-time Change Data Capture (CDC)
3ï¸âƒ£ Kafka Consumer    â†’  Converts events to Parquet & stores in MinIO
4ï¸âƒ£ Airflow DAG       â†’  Loads Parquet files to Snowflake RAW tables
5ï¸âƒ£ dbt Staging       â†’  Cleans & deduplicates data (Silver layer)
6ï¸âƒ£ dbt Snapshots     â†’  Tracks SCD Type 2 history
7ï¸âƒ£ dbt Marts         â†’  Business-ready dimensions & facts (Gold layer)
8ï¸âƒ£ Power BI          â†’  Interactive dashboards & reports
```

---

## ğŸ› ï¸ Tech Stack

<table>
<tr>
<td align="center" width="96">
  <a href="https://www.docker.com/">
    <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/docker/docker-original.svg" width="48" height="48" alt="Docker" />
  </a>
  <br><strong>Docker</strong>
</td>
<td align="center" width="96">
  <a href="https://www.postgresql.org/">
    <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/postgresql/postgresql-original.svg" width="48" height="48" alt="PostgreSQL" />
  </a>
  <br><strong>PostgreSQL</strong>
</td>
<td align="center" width="96">
  <a href="https://kafka.apache.org/">
    <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/apachekafka/apachekafka-original.svg" width="48" height="48" alt="Kafka" />
  </a>
  <br><strong>Kafka</strong>
</td>
<td align="center" width="96">
  <a href="https://debezium.io/">
    <img src="https://debezium.io/assets/images/color_white_debezium_type_600px.svg" width="48" height="48" alt="Debezium" />
  </a>
  <br><strong>Debezium</strong>
</td>
<td align="center" width="96">
  <a href="https://min.io/">
    <img src="https://raw.githubusercontent.com/minio/minio/master/.github/logo.svg" width="48" height="48" alt="MinIO" />
  </a>
  <br><strong>MinIO</strong>
</td>
</tr>
<tr>
<td align="center" width="96">
  <a href="https://www.snowflake.com/">
    <img src="https://www.vectorlogo.zone/logos/snowflake/snowflake-icon.svg" width="48" height="48" alt="Snowflake" />
  </a>
  <br><strong>Snowflake</strong>
</td>
<td align="center" width="96">
  <a href="https://www.getdbt.com/">
    <img src="https://seeklogo.com/images/D/dbt-logo-500AB0BAA7-seeklogo.com.png" width="48" height="48" alt="dbt" />
  </a>
  <br><strong>dbt</strong>
</td>
<td align="center" width="96">
  <a href="https://airflow.apache.org/">
    <img src="https://www.vectorlogo.zone/logos/apache_airflow/apache_airflow-icon.svg" width="48" height="48" alt="Airflow" />
  </a>
  <br><strong>Airflow</strong>
</td>
<td align="center" width="96">
  <a href="https://powerbi.microsoft.com/">
    <img src="https://upload.wikimedia.org/wikipedia/commons/c/cf/New_Power_BI_Logo.svg" width="48" height="48" alt="Power BI" />
  </a>
  <br><strong>Power BI</strong>
</td>
<td align="center" width="96">
  <a href="https://github.com/features/actions">
    <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/github/github-original.svg" width="48" height="48" alt="GitHub Actions" />
  </a>
  <br><strong>Actions</strong>
</td>
</tr>
</table>

### ğŸ“¦ Python Dependencies

| Package                      | Purpose                              |
| ---------------------------- | ------------------------------------ |
| `faker`                      | Generate realistic banking test data |
| `psycopg2-binary`            | PostgreSQL adapter                   |
| `kafka-python`               | Kafka consumer for CDC events        |
| `boto3`                      | S3-compatible storage (MinIO)        |
| `pandas` & `fastparquet`     | Data processing & Parquet I/O        |
| `dbt-snowflake`              | Data modeling & transformations      |
| `snowflake-connector-python` | Snowflake connectivity               |

---

## ğŸ“ Project Structure

```
ğŸ“¦ ModernDataStack
â”œâ”€â”€ ğŸ“‚ .github
â”‚   â””â”€â”€ ğŸ“‚ workflows
â”‚       â”œâ”€â”€ ğŸ“„ ci.yml                 # Lint, test, dbt compile
â”‚       â””â”€â”€ ğŸ“„ cd.yml                 # Deploy dbt models to prod
â”‚
â”œâ”€â”€ ğŸ“‚ banking_dbt                    # ğŸ”¶ dbt Project
â”‚   â”œâ”€â”€ ğŸ“„ dbt_project.yml
â”‚   â”œâ”€â”€ ğŸ“‚ models
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sources.yml            # Raw layer source definitions
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ staging                # ğŸ¥ˆ Silver Layer
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ stg_customers.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ stg_accounts.sql
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ stg_transactions.sql
â”‚   â”‚   â””â”€â”€ ğŸ“‚ marts                  # ğŸ¥‡ Gold Layer
â”‚   â”‚       â”œâ”€â”€ ğŸ“‚ dimensions
â”‚   â”‚       â”‚   â”œâ”€â”€ ğŸ“„ dim_customers.sql
â”‚   â”‚       â”‚   â””â”€â”€ ğŸ“„ dim_accounts.sql
â”‚   â”‚       â””â”€â”€ ğŸ“‚ facts
â”‚   â”‚           â””â”€â”€ ğŸ“„ fact_transactions.sql
â”‚   â””â”€â”€ ğŸ“‚ snapshots                  # ğŸ“¸ SCD Type 2
â”‚       â”œâ”€â”€ ğŸ“„ customers_snapshot.sql
â”‚       â””â”€â”€ ğŸ“„ accounts_snapshot.sql
â”‚
â”œâ”€â”€ ğŸ“‚ consumer
â”‚   â””â”€â”€ ğŸ“„ kafka-to-minio.py          # Kafka â†’ Parquet â†’ MinIO
â”‚
â”œâ”€â”€ ğŸ“‚ data-generator
â”‚   â””â”€â”€ ğŸ“„ faker_generator.py         # Fake banking data generator
â”‚
â”œâ”€â”€ ğŸ“‚ docker
â”‚   â”œâ”€â”€ ğŸ“‚ dags                       # Airflow DAGs
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ minio_to_snowflake.py  # Load raw data to Snowflake
â”‚   â”‚   â””â”€â”€ ğŸ“„ scd_snapshots.py       # Run dbt snapshots & marts
â”‚   â”œâ”€â”€ ğŸ“‚ logs                       # Airflow logs
â”‚   â”œâ”€â”€ ğŸ“‚ minio/data                 # MinIO object storage
â”‚   â”œâ”€â”€ ğŸ“‚ plugins                    # Airflow plugins
â”‚   â””â”€â”€ ğŸ“‚ postgres/data              # PostgreSQL data volume
â”‚
â”œâ”€â”€ ğŸ“‚ kafka-debezium
â”‚   â””â”€â”€ ğŸ“„ generate_and_post_connector.py  # Register Debezium connector
â”‚
â”œâ”€â”€ ğŸ“‚ postgres
â”‚   â””â”€â”€ ğŸ“„ schema.sql                 # Banking OLTP schema
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml             # ğŸ³ All services orchestration
â”œâ”€â”€ ğŸ“„ dockerfile-airflow.dockerfile  # Custom Airflow with dbt
â”œâ”€â”€ ğŸ“„ requirements.txt               # Python dependencies
â””â”€â”€ ğŸ“„ README.md                      # You are here!
```

---

## âš™ï¸ Setup Guide

### ğŸ“‹ Prerequisites

- ğŸ³ **Docker** & **Docker Compose**
- â„ï¸ **Snowflake Account** (with database `BANKING` and schema `RAW`, `ANALYTICS`)
- ğŸ **Python 3.11+** (for local development)

### 1ï¸âƒ£ Clone & Configure

```bash
# Clone the repository
git clone https://github.com/yourusername/ModernDataStack.git
cd ModernDataStack

# Create environment file
cp .env.example .env
```

<details>
<summary>ğŸ“ <strong>Required Environment Variables</strong></summary>

```env
# PostgreSQL (Source)
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=banking_user
POSTGRES_PASSWORD=your_password
POSTGRES_DB=banking

# Kafka
KAFKA_BOOTSTRAP=localhost:29092
KAFKA_GROUP=banking-consumer

# MinIO
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin
MINIO_ENDPOINT=http://localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=raw

# Airflow
AIRFLOW_DB_USER=airflow
AIRFLOW_DB_PASSWORD=airflow
AIRFLOW_DB_NAME=airflow

# Snowflake
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_user
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_DB=BANKING
SNOWFLAKE_SCHEMA=RAW
```

</details>

### 2ï¸âƒ£ Start the Stack

```bash
# Start all services
docker-compose up -d

# Verify services are running
docker-compose ps
```

| Service              | URL                   | Description               |
| -------------------- | --------------------- | ------------------------- |
| ğŸŒ **Airflow**       | http://localhost:8080 | Workflow UI (admin/admin) |
| ğŸ“¦ **MinIO**         | http://localhost:9001 | Object Storage Console    |
| ğŸ”Œ **Kafka Connect** | http://localhost:8083 | Debezium REST API         |
| ğŸ˜ **PostgreSQL**    | localhost:5432        | Source Database           |

### 3ï¸âƒ£ Initialize the Pipeline

```bash
# 1. Create the source database schema
psql -h localhost -U banking_user -d banking -f postgres/schema.sql

# 2. Register Debezium connector
python kafka-debezium/generate_and_post_connector.py

# 3. Start the Kafka consumer (in background)
python consumer/kafka-to-minio.py &

# 4. Generate fake banking data
python data-generator/faker_generator.py --once  # or without --once for continuous
```

### 4ï¸âƒ£ Configure dbt

```bash
# Create dbt profile
mkdir -p ~/.dbt
cat > ~/.dbt/profiles.yml <<EOF
banking_dbt:
  target: dev
  outputs:
    dev:
      type: snowflake
      account: {{ env_var('SNOWFLAKE_ACCOUNT') }}
      user: {{ env_var('SNOWFLAKE_USER') }}
      password: {{ env_var('SNOWFLAKE_PASSWORD') }}
      role: ACCOUNTADMIN
      database: BANKING
      warehouse: COMPUTE_WH
      schema: ANALYTICS
EOF

# Test connection
cd banking_dbt
dbt debug
dbt run
```

---

## ğŸ”„ Airflow DAGs

| DAG                          | Schedule     | Description                  |
| ---------------------------- | ------------ | ---------------------------- |
| `minio_to_snowflake_banking` | Every minute | Load Parquet â†’ Snowflake RAW |
| `SCD2_snapshots`             | Daily        | Run dbt snapshots + marts    |

---

## ğŸ§ª CI/CD Pipeline

### Continuous Integration (`ci.yml`)

```yaml
âœ“ Code checkout
âœ“ Python 3.11 setup
âœ“ Install dependencies
âœ“ Ruff linting
âœ“ Pytest unit tests
âœ“ dbt compile (validation)
```

### Continuous Deployment (`cd.yml`)

```yaml
âœ“ Code checkout
âœ“ Setup dbt profile (prod)
âœ“ dbt deps
âœ“ dbt run (production)
âœ“ dbt test (data quality)
```

---

## ğŸ¥‰ğŸ¥ˆğŸ¥‡ Medallion Architecture

| Layer         | Location          | Description                       |
| ------------- | ----------------- | --------------------------------- |
| ğŸ¥‰ **Bronze** | `BANKING.RAW`     | Raw Parquet data loaded as-is     |
| ğŸ¥ˆ **Silver** | `stg_*` views     | Cleaned, typed, deduplicated      |
| ğŸ¥‡ **Gold**   | `dim_*`, `fact_*` | Business-ready dimensions & facts |

### dbt Model Lineage

```
sources/raw.*
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Staging (Silver)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ stg_customers  â”‚ stg_accounts  â”‚ stg_transactions
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                â”‚                â”‚
        â–¼                â–¼                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  customers_   â”‚ â”‚  accounts_   â”‚        â”‚
â”‚  snapshot     â”‚ â”‚  snapshot    â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
        â”‚                â”‚                â”‚
        â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Marts (Gold)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  dim_customers  â”‚  dim_accounts  â”‚  fact_transactions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¸ SCD Type 2 Tracking

Both `customers` and `accounts` are tracked with **Slowly Changing Dimensions Type 2**:

```sql
-- Columns added by dbt snapshot
dbt_valid_from   -- When this version became active
dbt_valid_to     -- When this version was superseded (NULL = current)
is_current       -- Convenience flag for current record
```

---

## ğŸ§¹ Useful Commands

```bash
# Docker
docker-compose up -d          # Start all services
docker-compose down           # Stop all services
docker-compose logs -f kafka  # Follow Kafka logs

# dbt
cd banking_dbt
dbt deps                      # Install packages
dbt compile                   # Validate SQL
dbt run                       # Build models
dbt snapshot                  # Run SCD2 snapshots
dbt test                      # Run tests
dbt docs generate && dbt docs serve  # Documentation

# Data Generator
python data-generator/faker_generator.py --once  # Single batch
python data-generator/faker_generator.py         # Continuous loop
```

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“œ License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Built with â¤ï¸ using the Modern Data Stack**

<sub>PostgreSQL â€¢ Kafka â€¢ Debezium â€¢ MinIO â€¢ Snowflake â€¢ dbt â€¢ Airflow â€¢ Docker â€¢ GitHub Actions</sub>

</div>
